{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac513cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization, Conv2D, LeakyReLU, Lambda, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard, Callback\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "import cmath\n",
    "import time\n",
    "from utils import *\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "num = 2\n",
    "m = 16\n",
    "n = 16\n",
    "N_BS = m*n\n",
    "N_ms = 1\n",
    "cr = 3/8\n",
    "mtx_v = N_BS\n",
    "mtx_h = int(N_BS*cr)\n",
    "output_dim = int(N_BS*cr)\n",
    "\n",
    "SNR = [0, 5, 10, 15, 20]\n",
    "# data loading\n",
    "train = 'H_train.mat'\n",
    "val = 'H_val.mat'\n",
    "test = 'H_test.mat'\n",
    "import scipy.io\n",
    "# f = scipy.io.loadmat(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12330e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "m = 16\n",
    "n = 16\n",
    "N_BS = m*n\n",
    "N_ms = 1\n",
    "cr = 3/8\n",
    "mtx_v = N_BS\n",
    "mtx_h = int(N_BS*cr)\n",
    "output_dim = int(N_BS*cr)\n",
    "\n",
    "SNR = [0, 5, 10, 15, 20]\n",
    "# data loading\n",
    "train = 'H_train.mat'\n",
    "val = 'H_val.mat'\n",
    "test = 'H_test.mat'\n",
    "\n",
    "mat = scipy.io.loadmat(train)\n",
    "x_train = mat['H_train']\n",
    "# x_train = np.transpose(x_train, [2, 1, 0])\n",
    "x_train = x_train.astype('float32')  # 训练变量类型转换\n",
    "\n",
    "mat = scipy.io.loadmat(val)\n",
    "x_val = mat['H_val']\n",
    "# x_val = np.transpose(x_val, [2, 1, 0])\n",
    "x_val = x_val.astype('float32')  # 训练变量类型转换\n",
    "\n",
    "mat = scipy.io.loadmat(test)\n",
    "x_test = mat['H_test']\n",
    "# x_test = np.transpose(x_test, [2, 1, 0])\n",
    "x_test = x_test.astype('float32')  # 训练变量类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b52c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(train)\n",
    "x_train = mat['H_train']\n",
    "# x_train = np.transpose(x_train, [2, 1, 0])\n",
    "x_train = x_train.astype('float32')  # 训练变量类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea60758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85e74be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Mon Dec 06 20:59:31 2021',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'H_train': array([[[-0.18649173, -0.06178995, -0.13575908, ..., -0.53297448,\n",
       "          -0.19890173, -0.93904732],\n",
       "         [-0.04555401, -0.08338627,  0.23937207, ..., -0.3059061 ,\n",
       "          -0.08365838, -0.37805166]],\n",
       " \n",
       "        [[-0.18656522, -0.06233173, -0.14947567, ..., -0.51706836,\n",
       "          -0.19828873, -0.90770095],\n",
       "         [-0.06311223, -0.0865146 ,  0.23274629, ..., -0.32831382,\n",
       "          -0.08654447, -0.46782887]],\n",
       " \n",
       "        [[-0.18457913, -0.06256368, -0.16274771, ..., -0.49901663,\n",
       "          -0.19658102, -0.86477114],\n",
       "         [-0.08061889, -0.08990589,  0.22537006, ..., -0.34968805,\n",
       "          -0.08918391, -0.55311263]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.87411615,  0.86893041,  1.1944837 , ..., -1.42609207,\n",
       "          -0.81955793, -5.86435073],\n",
       "         [-0.03889684, -0.28846719, -0.4925879 , ..., -1.78246079,\n",
       "          -4.97849229,  8.2331077 ]],\n",
       " \n",
       "        [[ 0.90024762,  0.89447717,  1.22660021, ..., -1.41900949,\n",
       "          -0.7835763 , -6.00311735],\n",
       "         [ 0.04899949, -0.20158117, -0.3776372 , ..., -1.83299613,\n",
       "          -4.94072244,  8.18464464]],\n",
       " \n",
       "        [[ 0.91639636,  0.91070525,  1.24731676, ..., -1.40810029,\n",
       "          -0.75222217, -6.14273566],\n",
       "         [ 0.13873864, -0.11355775, -0.26161521, ..., -1.88528539,\n",
       "          -4.90706598,  8.13289455]]])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29c68d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = mat['H_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d38f5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.18649173, -0.06178995, -0.13575908, ..., -0.53297448,\n",
       "         -0.19890173, -0.93904732],\n",
       "        [-0.04555401, -0.08338627,  0.23937207, ..., -0.3059061 ,\n",
       "         -0.08365838, -0.37805166]],\n",
       "\n",
       "       [[-0.18656522, -0.06233173, -0.14947567, ..., -0.51706836,\n",
       "         -0.19828873, -0.90770095],\n",
       "        [-0.06311223, -0.0865146 ,  0.23274629, ..., -0.32831382,\n",
       "         -0.08654447, -0.46782887]],\n",
       "\n",
       "       [[-0.18457913, -0.06256368, -0.16274771, ..., -0.49901663,\n",
       "         -0.19658102, -0.86477114],\n",
       "        [-0.08061889, -0.08990589,  0.22537006, ..., -0.34968805,\n",
       "         -0.08918391, -0.55311263]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.87411615,  0.86893041,  1.1944837 , ..., -1.42609207,\n",
       "         -0.81955793, -5.86435073],\n",
       "        [-0.03889684, -0.28846719, -0.4925879 , ..., -1.78246079,\n",
       "         -4.97849229,  8.2331077 ]],\n",
       "\n",
       "       [[ 0.90024762,  0.89447717,  1.22660021, ..., -1.41900949,\n",
       "         -0.7835763 , -6.00311735],\n",
       "        [ 0.04899949, -0.20158117, -0.3776372 , ..., -1.83299613,\n",
       "         -4.94072244,  8.18464464]],\n",
       "\n",
       "       [[ 0.91639636,  0.91070525,  1.24731676, ..., -1.40810029,\n",
       "         -0.75222217, -6.14273566],\n",
       "        [ 0.13873864, -0.11355775, -0.26161521, ..., -1.88528539,\n",
       "         -4.90706598,  8.13289455]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46c5fc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512000, 2, 256)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da941c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512000/4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54cddd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 2, 256)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = scipy.io.loadmat(test)\n",
    "x_test = mat['H_test']\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e0c9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat(train)\n",
    "x_train = mat['H_train']\n",
    "x_train = np.transpose(x_train, [2, 1, 0])\n",
    "x_train = x_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bd58654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512000, 2, 256)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = scipy.io.loadmat(train)\n",
    "x_train = mat['H_train']\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "443afdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2, 512000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.transpose(x_test, [2, 1, 0])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea84f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(inputs, snr):\n",
    "    y_tmp = Dense(output_dim, activation='linear', use_bias=False)(inputs)\n",
    "    # y_tmp = MatMul(output_dim, mtx_v, mtx_h)(inputs)\n",
    "    y = Lambda(get_noise, arguments={'snr': snr})(y_tmp)\n",
    "    decode = Dense(N_BS, activation='linear', use_bias=False)(y)\n",
    "    # decode = MatMul(dictionary_dim, dictionary_mtx_v, dictionary_mtx_h)(y)\n",
    "    h_tmp = Reshape((num, m, n))(decode)\n",
    "    h = residual_block_decoded(h_tmp)\n",
    "    h = Reshape((num, N_BS))(h)\n",
    "    # h = Dense(N_BS, activation='linear')(h)\n",
    "    # h = Lambda(dic, arguments={'P_real_init': P_real, 'P_imag_init':P_imag})(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a4e19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 2, 256)]     0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2, 96)        24576       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 2, 96)        0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2, 256)       24576       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 2, 16, 16)    0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 8, 16, 16)    152         ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8, 16, 16)   64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 8, 16, 16)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 16, 16, 16)   1168        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 16, 16, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16, 16, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 16, 16)   4640        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 16, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 32, 16, 16)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 64, 16, 16)   18496       ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 16, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 64, 16, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 16, 16)  73856       ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128, 16, 16)  64         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 128, 16, 16)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 256, 16, 16)  295168      ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 256, 16, 16)  64         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 256, 16, 16)  0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 2, 16, 16)    4610        ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 2, 16, 16)   64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 2, 16, 16)    0           ['reshape[0][0]',                \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 2, 16, 16)    0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 2, 256)       0           ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 447,690\n",
      "Trainable params: 447,466\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "snr=0\n",
    "inpt = Input(shape=(num, N_BS,))\n",
    "outpt = network(inpt, snr)\n",
    "model = Model(inputs=[inpt], outputs=[outpt])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9c6734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses_train = []\n",
    "        self.losses_val = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses_train.append(logs.get('loss'))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses_val.append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba8f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = LossHistory()\n",
    "file = 'model_new_D' + '_cr_' + str(cr) + '_snr_' + str(snr)\n",
    "path = 'result_375/TensorBoard_%s' % file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1648a70b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "   4/4000 [..............................] - ETA: 1:13 - loss: 1.0417   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0072s vs `on_train_batch_end` time: 0.0138s). Check your callbacks.\n",
      "4000/4000 [==============================] - 92s 22ms/step - loss: 0.2097 - val_loss: 0.1713\n",
      "Epoch 2/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1638 - val_loss: 0.1639\n",
      "Epoch 3/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1582 - val_loss: 0.1618\n",
      "Epoch 4/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1551 - val_loss: 0.1612\n",
      "Epoch 5/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1529 - val_loss: 0.1577\n",
      "Epoch 6/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1513 - val_loss: 0.1568\n",
      "Epoch 7/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1501 - val_loss: 0.1570\n",
      "Epoch 8/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1492 - val_loss: 0.1564\n",
      "Epoch 9/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1483 - val_loss: 0.1553\n",
      "Epoch 10/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1477 - val_loss: 0.1556\n",
      "Epoch 11/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1471 - val_loss: 0.1547\n",
      "Epoch 12/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1465 - val_loss: 0.1539\n",
      "Epoch 13/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1461 - val_loss: 0.1559\n",
      "Epoch 14/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1457 - val_loss: 0.1538\n",
      "Epoch 15/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1453 - val_loss: 0.1542\n",
      "Epoch 16/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1449 - val_loss: 0.1536\n",
      "Epoch 17/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1448 - val_loss: 0.1543\n",
      "Epoch 18/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1444 - val_loss: 0.1528\n",
      "Epoch 19/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1442 - val_loss: 0.1527\n",
      "Epoch 20/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1440 - val_loss: 0.1529\n",
      "Epoch 21/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1438 - val_loss: 0.1521\n",
      "Epoch 22/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1435 - val_loss: 0.1523\n",
      "Epoch 23/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1434 - val_loss: 0.1518\n",
      "Epoch 24/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1432 - val_loss: 0.1521\n",
      "Epoch 25/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1430 - val_loss: 0.1519\n",
      "Epoch 26/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1428 - val_loss: 0.1517\n",
      "Epoch 27/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1427 - val_loss: 0.1520\n",
      "Epoch 28/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1425 - val_loss: 0.1524\n",
      "Epoch 29/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1424 - val_loss: 0.1536\n",
      "Epoch 30/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1422 - val_loss: 0.1515\n",
      "Epoch 31/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1421 - val_loss: 0.1517\n",
      "Epoch 32/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1421 - val_loss: 0.1509\n",
      "Epoch 33/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1419 - val_loss: 0.1517\n",
      "Epoch 34/300\n",
      "4000/4000 [==============================] - 93s 23ms/step - loss: 0.1418 - val_loss: 0.1511\n",
      "Epoch 35/300\n",
      "4000/4000 [==============================] - 97s 24ms/step - loss: 0.1417 - val_loss: 0.1532\n",
      "Epoch 36/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1415 - val_loss: 0.1509\n",
      "Epoch 37/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1414 - val_loss: 0.1512\n",
      "Epoch 38/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1415 - val_loss: 0.1516\n",
      "Epoch 39/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1412 - val_loss: 0.1526\n",
      "Epoch 40/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1412 - val_loss: 0.1515\n",
      "Epoch 41/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1411 - val_loss: 0.1513\n",
      "Epoch 42/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1410 - val_loss: 0.1514\n",
      "Epoch 43/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1409 - val_loss: 0.1519\n",
      "Epoch 44/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1408 - val_loss: 0.1514\n",
      "Epoch 45/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1407 - val_loss: 0.1509\n",
      "Epoch 46/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1407 - val_loss: 0.1514\n",
      "Epoch 47/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1405 - val_loss: 0.1508\n",
      "Epoch 48/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1404 - val_loss: 0.1515\n",
      "Epoch 49/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1403 - val_loss: 0.1508\n",
      "Epoch 50/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1403 - val_loss: 0.1519\n",
      "Epoch 51/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1402 - val_loss: 0.1506\n",
      "Epoch 52/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1401 - val_loss: 0.1526\n",
      "Epoch 53/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1402 - val_loss: 0.1515\n",
      "Epoch 54/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1400 - val_loss: 0.1508\n",
      "Epoch 55/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1400 - val_loss: 0.1510\n",
      "Epoch 56/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1400 - val_loss: 0.1508\n",
      "Epoch 57/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1399 - val_loss: 0.1509\n",
      "Epoch 58/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1398 - val_loss: 0.1516\n",
      "Epoch 59/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1397 - val_loss: 0.1506\n",
      "Epoch 60/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1397 - val_loss: 0.1522\n",
      "Epoch 61/300\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.1397 - val_loss: 0.1504\n",
      "Epoch 62/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1396 - val_loss: 0.1511\n",
      "Epoch 63/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1395 - val_loss: 0.1509\n",
      "Epoch 64/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1394 - val_loss: 0.1509\n",
      "Epoch 65/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1394 - val_loss: 0.1523\n",
      "Epoch 66/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1394 - val_loss: 0.1505\n",
      "Epoch 67/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1393 - val_loss: 0.1506\n",
      "Epoch 68/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1393 - val_loss: 0.1518\n",
      "Epoch 69/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1393 - val_loss: 0.1508\n",
      "Epoch 70/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1392 - val_loss: 0.1510\n",
      "Epoch 71/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1392 - val_loss: 0.1503\n",
      "Epoch 72/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1391 - val_loss: 0.1505\n",
      "Epoch 73/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1392 - val_loss: 0.1507\n",
      "Epoch 74/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1391 - val_loss: 0.1509\n",
      "Epoch 75/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1390 - val_loss: 0.1503\n",
      "Epoch 76/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1390 - val_loss: 0.1510\n",
      "Epoch 77/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1388 - val_loss: 0.1514\n",
      "Epoch 78/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1389 - val_loss: 0.1510\n",
      "Epoch 79/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1388 - val_loss: 0.1503\n",
      "Epoch 80/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1387 - val_loss: 0.1502\n",
      "Epoch 81/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1388 - val_loss: 0.1508\n",
      "Epoch 82/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1387 - val_loss: 0.1508\n",
      "Epoch 83/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1387 - val_loss: 0.1507\n",
      "Epoch 84/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1387 - val_loss: 0.1510\n",
      "Epoch 85/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1385 - val_loss: 0.1515\n",
      "Epoch 86/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1386 - val_loss: 0.1504\n",
      "Epoch 87/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1385 - val_loss: 0.1514\n",
      "Epoch 88/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1385 - val_loss: 0.1502\n",
      "Epoch 89/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1385 - val_loss: 0.1510\n",
      "Epoch 90/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1385 - val_loss: 0.1518\n",
      "Epoch 91/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1383 - val_loss: 0.1508\n",
      "Epoch 92/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1384 - val_loss: 0.1515\n",
      "Epoch 93/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1383 - val_loss: 0.1509\n",
      "Epoch 94/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1383 - val_loss: 0.1507\n",
      "Epoch 95/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1384 - val_loss: 0.1508\n",
      "Epoch 96/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1383 - val_loss: 0.1507\n",
      "Epoch 97/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1382 - val_loss: 0.1515\n",
      "Epoch 98/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1382 - val_loss: 0.1502\n",
      "Epoch 99/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1382 - val_loss: 0.1504\n",
      "Epoch 100/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1382 - val_loss: 0.1517\n",
      "Epoch 101/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1381 - val_loss: 0.1508\n",
      "Epoch 102/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1382 - val_loss: 0.1506\n",
      "Epoch 103/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1381 - val_loss: 0.1505\n",
      "Epoch 104/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1380 - val_loss: 0.1504\n",
      "Epoch 105/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1380 - val_loss: 0.1514\n",
      "Epoch 106/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1380 - val_loss: 0.1509\n",
      "Epoch 107/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1378 - val_loss: 0.1506\n",
      "Epoch 108/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1379 - val_loss: 0.1511\n",
      "Epoch 109/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1379 - val_loss: 0.1506\n",
      "Epoch 110/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1378 - val_loss: 0.1505\n",
      "Epoch 111/300\n",
      "4000/4000 [==============================] - 92s 23ms/step - loss: 0.1379 - val_loss: 0.1507\n",
      "Epoch 112/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1378 - val_loss: 0.1519\n",
      "Epoch 113/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1378 - val_loss: 0.1538\n",
      "Epoch 114/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1378 - val_loss: 0.1507\n",
      "Epoch 115/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1378 - val_loss: 0.1505\n",
      "Epoch 116/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1377 - val_loss: 0.1506\n",
      "Epoch 117/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1377 - val_loss: 0.1517\n",
      "Epoch 118/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1378 - val_loss: 0.1510\n",
      "Epoch 119/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1377 - val_loss: 0.1505\n",
      "Epoch 120/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1377 - val_loss: 0.1509\n",
      "Epoch 121/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1377 - val_loss: 0.1528\n",
      "Epoch 122/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1376 - val_loss: 0.1506\n",
      "Epoch 123/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1376 - val_loss: 0.1513\n",
      "Epoch 124/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1375 - val_loss: 0.1510\n",
      "Epoch 125/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1375 - val_loss: 0.1511\n",
      "Epoch 126/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1374 - val_loss: 0.1505\n",
      "Epoch 127/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1375 - val_loss: 0.1510\n",
      "Epoch 128/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1374 - val_loss: 0.1506\n",
      "Epoch 129/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1375 - val_loss: 0.1510\n",
      "Epoch 130/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1374 - val_loss: 0.1510\n",
      "Epoch 131/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1373 - val_loss: 0.1518\n",
      "Epoch 132/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1374 - val_loss: 0.1516\n",
      "Epoch 133/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1374 - val_loss: 0.1503\n",
      "Epoch 134/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1373 - val_loss: 0.1507\n",
      "Epoch 135/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1374 - val_loss: 0.1507\n",
      "Epoch 136/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1373 - val_loss: 0.1504\n",
      "Epoch 137/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1373 - val_loss: 0.1506\n",
      "Epoch 138/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1372 - val_loss: 0.1507\n",
      "Epoch 139/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1371 - val_loss: 0.1507\n",
      "Epoch 140/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1373 - val_loss: 0.1507\n",
      "Epoch 141/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1372 - val_loss: 0.1507\n",
      "Epoch 142/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1372 - val_loss: 0.1506\n",
      "Epoch 143/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1372 - val_loss: 0.1537\n",
      "Epoch 144/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1371 - val_loss: 0.1507\n",
      "Epoch 145/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1371 - val_loss: 0.1510\n",
      "Epoch 146/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1371 - val_loss: 0.1506\n",
      "Epoch 147/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1371 - val_loss: 0.1514\n",
      "Epoch 148/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1371 - val_loss: 0.1510\n",
      "Epoch 149/300\n",
      "4000/4000 [==============================] - 92s 23ms/step - loss: 0.1371 - val_loss: 0.1510\n",
      "Epoch 150/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1370 - val_loss: 0.1515\n",
      "Epoch 151/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1371 - val_loss: 0.1512\n",
      "Epoch 152/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1371 - val_loss: 0.1510\n",
      "Epoch 153/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1370 - val_loss: 0.1512\n",
      "Epoch 154/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1370 - val_loss: 0.1508\n",
      "Epoch 155/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1370 - val_loss: 0.1514\n",
      "Epoch 156/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1369 - val_loss: 0.1506\n",
      "Epoch 157/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1369 - val_loss: 0.1509\n",
      "Epoch 158/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1369 - val_loss: 0.1510\n",
      "Epoch 159/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1369 - val_loss: 0.1505\n",
      "Epoch 160/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1369 - val_loss: 0.1511\n",
      "Epoch 161/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1369 - val_loss: 0.1508\n",
      "Epoch 162/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1368 - val_loss: 0.1510\n",
      "Epoch 163/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1368 - val_loss: 0.1509\n",
      "Epoch 164/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1369 - val_loss: 0.1515\n",
      "Epoch 165/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1368 - val_loss: 0.1518\n",
      "Epoch 166/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1368 - val_loss: 0.1512\n",
      "Epoch 167/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1367 - val_loss: 0.1507\n",
      "Epoch 168/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1368 - val_loss: 0.1501\n",
      "Epoch 169/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1367 - val_loss: 0.1510\n",
      "Epoch 170/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1367 - val_loss: 0.1506\n",
      "Epoch 171/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1366 - val_loss: 0.1511\n",
      "Epoch 172/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1368 - val_loss: 0.1514\n",
      "Epoch 173/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1367 - val_loss: 0.1517\n",
      "Epoch 174/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1367 - val_loss: 0.1507\n",
      "Epoch 175/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1367 - val_loss: 0.1514\n",
      "Epoch 176/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1367 - val_loss: 0.1515\n",
      "Epoch 177/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1367 - val_loss: 0.1514\n",
      "Epoch 178/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1366 - val_loss: 0.1511\n",
      "Epoch 179/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1365 - val_loss: 0.1510\n",
      "Epoch 180/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1366 - val_loss: 0.1513\n",
      "Epoch 181/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1365 - val_loss: 0.1515\n",
      "Epoch 182/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1365 - val_loss: 0.1513\n",
      "Epoch 183/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1365 - val_loss: 0.1518\n",
      "Epoch 184/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1365 - val_loss: 0.1514\n",
      "Epoch 185/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1365 - val_loss: 0.1510\n",
      "Epoch 186/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1365 - val_loss: 0.1506\n",
      "Epoch 187/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1365 - val_loss: 0.1505\n",
      "Epoch 188/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1365 - val_loss: 0.1512\n",
      "Epoch 189/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1365 - val_loss: 0.1511\n",
      "Epoch 190/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1364 - val_loss: 0.1519\n",
      "Epoch 191/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1364 - val_loss: 0.1509\n",
      "Epoch 192/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1364 - val_loss: 0.1510\n",
      "Epoch 193/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1507\n",
      "Epoch 194/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1364 - val_loss: 0.1507\n",
      "Epoch 195/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1506\n",
      "Epoch 196/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1506\n",
      "Epoch 197/300\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.1364 - val_loss: 0.1518\n",
      "Epoch 198/300\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.1363 - val_loss: 0.1509\n",
      "Epoch 199/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1364 - val_loss: 0.1521\n",
      "Epoch 200/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1363 - val_loss: 0.1511\n",
      "Epoch 201/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1363 - val_loss: 0.1518\n",
      "Epoch 202/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1363 - val_loss: 0.1514\n",
      "Epoch 203/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1508\n",
      "Epoch 204/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1362 - val_loss: 0.1514\n",
      "Epoch 205/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1507\n",
      "Epoch 206/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1510\n",
      "Epoch 207/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1362 - val_loss: 0.1513\n",
      "Epoch 208/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1512\n",
      "Epoch 209/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1362 - val_loss: 0.1515\n",
      "Epoch 210/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1512\n",
      "Epoch 211/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1362 - val_loss: 0.1508\n",
      "Epoch 212/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1362 - val_loss: 0.1512\n",
      "Epoch 213/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1363 - val_loss: 0.1509\n",
      "Epoch 214/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1361 - val_loss: 0.1514\n",
      "Epoch 215/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1361 - val_loss: 0.1513\n",
      "Epoch 216/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1361 - val_loss: 0.1511\n",
      "Epoch 217/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1361 - val_loss: 0.1509\n",
      "Epoch 218/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1361 - val_loss: 0.1511\n",
      "Epoch 219/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1362 - val_loss: 0.1511\n",
      "Epoch 220/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1360 - val_loss: 0.1515\n",
      "Epoch 221/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1361 - val_loss: 0.1520\n",
      "Epoch 222/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1516\n",
      "Epoch 223/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1361 - val_loss: 0.1514\n",
      "Epoch 224/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1361 - val_loss: 0.1511\n",
      "Epoch 225/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1361 - val_loss: 0.1514\n",
      "Epoch 226/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1513\n",
      "Epoch 227/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1509\n",
      "Epoch 228/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1361 - val_loss: 0.1518\n",
      "Epoch 229/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1512\n",
      "Epoch 230/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1509\n",
      "Epoch 231/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1514\n",
      "Epoch 232/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1361 - val_loss: 0.1507\n",
      "Epoch 233/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1508\n",
      "Epoch 234/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1520\n",
      "Epoch 235/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1360 - val_loss: 0.1516\n",
      "Epoch 236/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1359 - val_loss: 0.1512\n",
      "Epoch 237/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1359 - val_loss: 0.1510\n",
      "Epoch 238/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1359 - val_loss: 0.1508\n",
      "Epoch 239/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1359 - val_loss: 0.1518\n",
      "Epoch 240/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1358 - val_loss: 0.1512\n",
      "Epoch 241/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1359 - val_loss: 0.1511\n",
      "Epoch 242/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1359 - val_loss: 0.1511\n",
      "Epoch 243/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1358 - val_loss: 0.1511\n",
      "Epoch 244/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1358 - val_loss: 0.1515\n",
      "Epoch 245/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1358 - val_loss: 0.1517\n",
      "Epoch 246/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1359 - val_loss: 0.1513\n",
      "Epoch 247/300\n",
      "4000/4000 [==============================] - 90s 23ms/step - loss: 0.1358 - val_loss: 0.1516\n",
      "Epoch 248/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1358 - val_loss: 0.1508\n",
      "Epoch 249/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1358 - val_loss: 0.1516\n",
      "Epoch 250/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1357 - val_loss: 0.1510\n",
      "Epoch 251/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1358 - val_loss: 0.1510\n",
      "Epoch 252/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1358 - val_loss: 0.1512\n",
      "Epoch 253/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1357 - val_loss: 0.1510\n",
      "Epoch 254/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1357 - val_loss: 0.1514\n",
      "Epoch 255/300\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.1357 - val_loss: 0.1510\n",
      "Epoch 256/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1357 - val_loss: 0.1516\n",
      "Epoch 257/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1358 - val_loss: 0.1512\n",
      "Epoch 258/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1357 - val_loss: 0.1513\n",
      "Epoch 259/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1357 - val_loss: 0.1518\n",
      "Epoch 260/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1358 - val_loss: 0.1516\n",
      "Epoch 261/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1358 - val_loss: 0.1519\n",
      "Epoch 262/300\n",
      "4000/4000 [==============================] - 86s 22ms/step - loss: 0.1356 - val_loss: 0.1516\n",
      "Epoch 263/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1357 - val_loss: 0.1513\n",
      "Epoch 264/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1356 - val_loss: 0.1517\n",
      "Epoch 265/300\n",
      "4000/4000 [==============================] - 85s 21ms/step - loss: 0.1356 - val_loss: 0.1526\n",
      "Epoch 266/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1356 - val_loss: 0.1512\n",
      "Epoch 267/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1356 - val_loss: 0.1512\n",
      "Epoch 268/300\n",
      "4000/4000 [==============================] - 91s 23ms/step - loss: 0.1356 - val_loss: 0.1518\n",
      "Epoch 269/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1357 - val_loss: 0.1518\n",
      "Epoch 270/300\n",
      "4000/4000 [==============================] - 102s 25ms/step - loss: 0.1356 - val_loss: 0.1512\n",
      "Epoch 271/300\n",
      "4000/4000 [==============================] - 103s 26ms/step - loss: 0.1356 - val_loss: 0.1518\n",
      "Epoch 272/300\n",
      "4000/4000 [==============================] - 103s 26ms/step - loss: 0.1355 - val_loss: 0.1508\n",
      "Epoch 273/300\n",
      "4000/4000 [==============================] - 103s 26ms/step - loss: 0.1357 - val_loss: 0.1515\n",
      "Epoch 274/300\n",
      "4000/4000 [==============================] - 103s 26ms/step - loss: 0.1355 - val_loss: 0.1519\n",
      "Epoch 275/300\n",
      "4000/4000 [==============================] - 104s 26ms/step - loss: 0.1356 - val_loss: 0.1510\n",
      "Epoch 276/300\n",
      "4000/4000 [==============================] - 103s 26ms/step - loss: 0.1356 - val_loss: 0.1520\n",
      "Epoch 277/300\n",
      "4000/4000 [==============================] - 102s 26ms/step - loss: 0.1356 - val_loss: 0.1521\n",
      "Epoch 278/300\n",
      "4000/4000 [==============================] - 104s 26ms/step - loss: 0.1356 - val_loss: 0.1519\n",
      "Epoch 279/300\n",
      "4000/4000 [==============================] - 104s 26ms/step - loss: 0.1356 - val_loss: 0.1512\n",
      "Epoch 280/300\n",
      "4000/4000 [==============================] - 101s 25ms/step - loss: 0.1354 - val_loss: 0.1510\n",
      "Epoch 281/300\n",
      "4000/4000 [==============================] - 87s 22ms/step - loss: 0.1355 - val_loss: 0.1508\n",
      "Epoch 282/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1355 - val_loss: 0.1520\n",
      "Epoch 283/300\n",
      "4000/4000 [==============================] - 90s 22ms/step - loss: 0.1355 - val_loss: 0.1506\n",
      "Epoch 284/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1355 - val_loss: 0.1519\n",
      "Epoch 285/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1355 - val_loss: 0.1510\n",
      "Epoch 286/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1355 - val_loss: 0.1512\n",
      "Epoch 287/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1354 - val_loss: 0.1513\n",
      "Epoch 288/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1354 - val_loss: 0.1513\n",
      "Epoch 289/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1354 - val_loss: 0.1516\n",
      "Epoch 290/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1355 - val_loss: 0.1522\n",
      "Epoch 291/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1354 - val_loss: 0.1516\n",
      "Epoch 292/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1354 - val_loss: 0.1515\n",
      "Epoch 293/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1354 - val_loss: 0.1510\n",
      "Epoch 294/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1354 - val_loss: 0.1515\n",
      "Epoch 295/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1354 - val_loss: 0.1512\n",
      "Epoch 296/300\n",
      "4000/4000 [==============================] - 88s 22ms/step - loss: 0.1353 - val_loss: 0.1515\n",
      "Epoch 297/300\n",
      "4000/4000 [==============================] - 89s 22ms/step - loss: 0.1354 - val_loss: 0.1515\n",
      "Epoch 298/300\n",
      "4000/4000 [==============================] - 86s 21ms/step - loss: 0.1353 - val_loss: 0.1511\n",
      "Epoch 299/300\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 0.1354 - val_loss: 0.1515\n",
      "Epoch 300/300\n",
      "4000/4000 [==============================] - 84s 21ms/step - loss: 0.1354 - val_loss: 0.1508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e82c4aea60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, x_train,\n",
    "          epochs=300,\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_val, x_val),\n",
    "          callbacks=[history,\n",
    "                     TensorBoard(log_dir=path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "705b6108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It cost 0.000113 sec\n",
      "When compress rate is 0.375\n",
      "SNR is 0 dB\n",
      "NMSE is  -5.033408228199624 dB\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'result_375/saved_model/model_new_D_cr_0.375_snr_0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16336/259424849.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmodel_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0moutfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"result_375/saved_model/%s.json\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m    \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# serialize weights to HDF5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'result_375/saved_model/model_new_D_cr_0.375_snr_0.json'"
     ]
    }
   ],
   "source": [
    "tStart = time.time()\n",
    "x_hat = model.predict(x_test)\n",
    "tEnd = time.time()\n",
    "print(\"It cost %f sec\" % ((tEnd - tStart) / x_test.shape[0]))\n",
    "\n",
    "x_re = x_test[:, 0, :]\n",
    "x_im = x_test[:, 1, :]\n",
    "x_test_c = x_re + 1j * x_im\n",
    "\n",
    "x_hat_re = x_hat[:, 0, :]\n",
    "x_hat_im = x_hat[:, 1, :]\n",
    "x_hat_c = x_hat_re + 1j * x_hat_im\n",
    "\n",
    "power = np.sum(abs(x_test_c) ** 2, axis=1)\n",
    "mse = np.sum(abs(x_test_c - x_hat_c) ** 2, axis=1)\n",
    "\n",
    "print(\"When compress rate is\", cr)\n",
    "print('SNR is', snr, 'dB')\n",
    "print(\"NMSE is \", 10 * math.log10(np.mean(mse / power)), 'dB')\n",
    "'''\n",
    "filename = \"result/model_%s.csv\" % file\n",
    "x_hat1 = np.reshape(x_hat, (len(x_hat), -1))\n",
    "np.savetxt(filename, x_hat1, delimiter=\",\")\n",
    "'''\n",
    "model_json = model.to_json()\n",
    "outfile = \"result_375/saved_model/%s.json\" % file\n",
    "with open(outfile, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "outfile = \"result_375/saved_model/%s.h5\" % file\n",
    "model.save_weights(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45adfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "outfile = \"result_375/saved_model/%s.json\" % file\n",
    "with open(outfile, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "outfile = \"result_375/saved_model/%s.h5\" % file\n",
    "model.save_weights(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9c885b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_re = x_test[:, 0, :]\n",
    "x_im = x_test[:, 1, :]\n",
    "x_test_c = x_re + 1j * x_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a940020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128000, 256)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "600ae0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "power = np.sum(abs(x_test_c) ** 2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f46620d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135803.67   , 125208.99   , 120958.81   , 172062.06   ,\n",
       "       148947.42   , 216666.75   , 352411.3    ,  36361.492  ,\n",
       "        14966.121  ,  27756.934  , 227355.61   , 263872.97   ,\n",
       "       163634.77   , 130533.03   , 163753.95   , 176087.27   ,\n",
       "       106749.55   , 140258.58   , 182236.53   , 156282.52   ,\n",
       "       233533.61   , 285707.72   , 273399.4    ,  31674.71   ,\n",
       "        15849.468  ,  32319.727  , 256499.28   , 227838.66   ,\n",
       "       192964.48   , 159700.16   , 212183.8    , 106817.3    ,\n",
       "       169967.77   , 117781.66   , 131232.42   , 212068.72   ,\n",
       "       276504.97   , 357342.2    , 316745.75   ,  38707.21   ,\n",
       "        16907.428  ,  31814.375  , 243307.5    , 386589.97   ,\n",
       "       188397.77   , 145539.     , 236014.94   , 151589.58   ,\n",
       "       173698.5    , 120823.9    , 175691.62   , 234501.53   ,\n",
       "       275756.06   , 249761.81   , 282376.22   ,  35174.35   ,\n",
       "        18439.365  ,  39893.383  , 411013.3    , 281122.8    ,\n",
       "       301631.3    , 172372.89   , 118842.69   , 136293.05   ,\n",
       "       202074.64   , 157548.52   , 220126.73   , 192975.92   ,\n",
       "       179296.17   , 215352.9    , 159806.58   ,  26738.361  ,\n",
       "        13958.93   ,  23592.38   , 256980.36   , 265710.4    ,\n",
       "       247951.12   , 175738.12   , 143602.69   , 147968.23   ,\n",
       "       233624.47   , 147858.89   , 186990.11   , 265138.25   ,\n",
       "       333240.5    , 159954.88   ,  35110.543  ,  10819.39   ,\n",
       "         8395.403  ,  11323.757  ,  52093.246  , 181648.6    ,\n",
       "       257790.28   , 265964.28   , 145797.67   , 164215.14   ,\n",
       "       133767.03   , 212455.72   , 250963.84   , 119237.84   ,\n",
       "        49363.266  ,  21865.486  ,  13338.3545 ,   4414.116  ,\n",
       "         3540.6143 ,   4301.031  ,  16088.686  ,  24288.238  ,\n",
       "        56335.668  ,  76498.21   , 160067.16   , 154251.02   ,\n",
       "        30632.021  ,  22725.438  ,  15850.6455 ,  14663.683  ,\n",
       "        12423.932  ,  10303.566  ,   7286.679  ,   1202.0746 ,\n",
       "          735.7078 ,   1177.963  ,   8613.295  ,  10597.608  ,\n",
       "        12960.153  ,  11806.898  ,  14374.833  ,  18069.781  ,\n",
       "        12612.107  ,  10296.951  ,  10416.893  ,   9818.0205 ,\n",
       "         9522.858  ,   9246.357  ,   6598.255  ,    967.48193,\n",
       "          555.2792 ,    942.3458 ,   7433.6216 ,   8653.136  ,\n",
       "        10438.731  ,   9501.568  ,   8472.358  ,  10557.424  ,\n",
       "        19977.762  ,  13864.342  ,  15258.574  ,  11730.493  ,\n",
       "        11641.4375 ,  11268.937  ,   7478.6235 ,   1136.1172 ,\n",
       "          681.27765,   1105.767  ,   8148.5117 ,   9661.786  ,\n",
       "        13635.582  ,  13848.512  ,  11071.018  ,  19403.936  ,\n",
       "       195877.42   , 124381.37   , 158248.14   ,  77629.25   ,\n",
       "        41539.7    ,  25208.67   ,  13789.556  ,   4128.2744 ,\n",
       "         3357.6392 ,   4123.329  ,  14503.685  ,  20405.559  ,\n",
       "        51636.64   ,  97693.21   , 137691.48   , 164782.6    ,\n",
       "       169917.81   , 146702.47   , 204801.94   , 146363.52   ,\n",
       "       219389.05   , 236359.8    ,  43076.31   ,  11522.6875 ,\n",
       "         8615.487  ,  10820.23   ,  40805.85   , 145983.7    ,\n",
       "       317342.47   , 220223.34   , 138604.19   , 174800.86   ,\n",
       "       158125.34   , 134043.52   , 203367.34   , 174318.83   ,\n",
       "       257901.6    , 343823.06   , 232244.97   ,  25970.092  ,\n",
       "        15168.441  ,  23896.037  , 281444.62   , 217018.8    ,\n",
       "       265466.6    , 136661.98   , 139595.64   , 149788.25   ,\n",
       "       161349.88   , 141095.23   , 141693.34   , 179968.27   ,\n",
       "       191265.97   , 251642.11   , 237729.64   ,  32915.625  ,\n",
       "        17208.19   ,  41643.625  , 462297.06   , 220617.55   ,\n",
       "       194608.92   , 209485.8    , 122960.66   , 135088.75   ,\n",
       "       214870.61   , 167005.08   , 149603.4    , 163119.66   ,\n",
       "       170588.31   , 301092.03   , 205145.25   ,  25277.707  ,\n",
       "        14048.692  ,  26585.254  , 177489.19   , 184992.75   ,\n",
       "       212193.12   , 172968.6    , 145619.17   , 145624.03   ,\n",
       "       157156.31   , 163509.56   , 144591.56   , 153564.33   ,\n",
       "       228402.     , 248075.03   , 283437.1    ,  29123.387  ,\n",
       "        14835.136  ,  27933.043  , 217251.3    , 269909.6    ,\n",
       "       263517.28   , 265572.1    , 158009.92   , 222440.33   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d418a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.sum(abs(x_test_c - 0) ** 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cc86f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_re = x_test[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aed1965b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 128000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_re.transpose().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ae9bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as k\n",
    "from keras.models import model_from_json\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from utils import *\n",
    "cr = 1/2\n",
    "NMSE = []\n",
    "SNR = [0, 5, 10, 15, 20]\n",
    "# SNR = [0, 5, 15, 20]\n",
    "snr=0\n",
    "mat = scipy.io.loadmat(test)\n",
    "x_test = mat['H_test']\n",
    "# x_test = np.transpose(x_test, [2, 1, 0])\n",
    "x_test = x_test.astype('float32')  # 训练变量类型转换\n",
    "# test = 'old_375/H_test.mat'\n",
    "# mat = h5py.File(test)\n",
    "# x_test = mat['H_test']\n",
    "# x_test = np.transpose(x_test, [2, 1, 0])\n",
    "# x_test = x_test.astype('float32')  # 训练变量类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4597ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr=0.375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a451890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR is 0 dB\n",
      "NMSE is  -5.031026136367213 dB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file = 'model_new_D' + '_cr_' + str(cr) + '_snr_' + str(snr)\n",
    "outfile = \"result_375/saved_model/%s.json\" % file\n",
    "# outfile = \"old_375/%s.json\" % file\n",
    "json_file = open(outfile, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "net = model_from_json(loaded_model_json, custom_objects={'k': k})\n",
    "outfile = \"result_375/saved_model/%s.h5\" % file\n",
    "# outfile = \"old_375/%s.h5\" % file\n",
    "net.load_weights(outfile)\n",
    "\n",
    "x_hat = net.predict(x_test)\n",
    "\n",
    "x_re = x_test[:, 0, :]\n",
    "x_im = x_test[:, 1, :]\n",
    "x_test_c = x_re + 1j * x_im\n",
    "\n",
    "x_hat_re = x_hat[:, 0, :]\n",
    "x_hat_im = x_hat[:, 1, :]\n",
    "x_hat_c = x_hat_re + 1j * x_hat_im\n",
    "\n",
    "power = np.sum(abs(x_test_c) ** 2, axis=1)\n",
    "mse = np.sum(abs(x_test_c - x_hat_c) ** 2, axis=1)\n",
    "nmse = 10*math.log10(np.mean(mse/power))\n",
    "print('SNR is', snr, 'dB')\n",
    "print(\"NMSE is \", nmse, 'dB')\n",
    "NMSE.append(nmse)\n",
    "\n",
    "sio.savemat('NMSE_DL_4_path_cr_'+ str(cr) + '.mat', mdict={'NMSE_DL_4_path_2': NMSE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903bd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
